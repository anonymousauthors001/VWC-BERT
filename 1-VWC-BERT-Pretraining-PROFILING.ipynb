{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: Tesla P100-PCIE-16GB\n",
      "cuda\n",
      "Cpu count:  20\n"
     ]
    }
   ],
   "source": [
    "import torch# If there's a GPU available...\n",
    "import random\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" ##I will find a way to fix this later :(\n",
    "\n",
    "NUM_GPUS=0\n",
    "\n",
    "try:\n",
    "    if torch.cuda.is_available():  \n",
    "        device = torch.device(\"cuda\")\n",
    "        NUM_GPUS=torch.cuda.device_count()\n",
    "        print('There are %d GPU(s) available.' % NUM_GPUS)\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name())# If not...\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")  \n",
    "except:\n",
    "    print('Cuda error using CPU instead.')\n",
    "    device = torch.device(\"cpu\")  \n",
    "    \n",
    "print(device)\n",
    "\n",
    "# device = torch.device(\"cpu\")  \n",
    "# print(device)\n",
    "\n",
    "NUM_PROCESSORS=multiprocessing.cpu_count()\n",
    "print(\"Cpu count: \",NUM_PROCESSORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading directory:  ./Results\n",
      "Model Saving directory: ./Results/NVD/Model/\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.Dataset import getDataset, getDummyDataset, Data        \n",
    "\n",
    "DIR='./Results'\n",
    "    \n",
    "from pathlib import Path\n",
    "Path(DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATASET_LOAD_DIR=\"./NVD/\"\n",
    "MODEL_SAVE_DIR=DIR+'/NVD/Model/'\n",
    "\n",
    "Path(MODEL_SAVE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "print(\"Data loading directory: \", DIR)\n",
    "print(\"Model Saving directory:\", MODEL_SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some more packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "import zipfile\n",
    "import wget\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import RandomSampler,SequentialSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.tuner.tuning import Tuner\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For reproduciblity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Set the seed value all over the place to make this reproducible.\n",
    "from random import sample\n",
    "\n",
    "seed_val = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_val)\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "pl.seed_everything(seed_val)\n",
    "\n",
    "try:\n",
    "    torch.cuda.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "except:\n",
    "    print(\"nothing to set for cudnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible choices for pretrained are:\n",
    "distilbert-base-uncased\n",
    "bert-base-uncased\n",
    "\n",
    "The BERT paper says: \"[The] pre-trained BERT model can be fine-tuned with just one additional output\n",
    "layer to create state-of-the-art models for a wide range of tasks, such as question answering and\n",
    "language inference, without substantial task-specific architecture modifications.\"\n",
    "\n",
    "Huggingface/transformers provides access to such pretrained model versions, some of which have been\n",
    "published by various community members.\n",
    "\n",
    "BertForSequenceClassification is one of those pretrained models, which is loaded automatically by\n",
    "AutoModelForSequenceClassification because it corresponds to the pretrained weights of\n",
    "\"bert-base-uncased\".\n",
    "\n",
    "Huggingface says about BertForSequenceClassification: Bert Model transformer with a sequence\n",
    "classification/regression head on top (a linear layer on top of the pooled output) e.g. for GLUE\n",
    "tasks.\"\n",
    "\n",
    "This part is easy  we instantiate the pretrained model (checkpoint)\n",
    "\n",
    "But it's also incredibly important, e.g. by using \"bert-base-uncased, we determine, that that model\n",
    "does not distinguish between lower and upper case. This might have a significant impact on model\n",
    "performance!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self,*args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        # a very useful feature of pytorch lightning  which leads to the named variables that are passed in\n",
    "        # being available as self.hparams.<variable_name> We use this when refering to eg\n",
    "        # self.hparams.learning_rate\n",
    "\n",
    "        # freeze\n",
    "        self._frozen = False\n",
    "\n",
    "        # eg https://github.com/stefan-it/turkish-bert/issues/5\n",
    "        config = AutoConfig.from_pretrained(self.hparams.pretrained,\n",
    "                                            output_attentions=False,\n",
    "                                            output_hidden_states=False)\n",
    "\n",
    "        #print(config)\n",
    "\n",
    "        A = AutoModelForMaskedLM\n",
    "        self.model = A.from_pretrained(self.hparams.pretrained, config=config)\n",
    "\n",
    "        print('Model: ', type(self.model))\n",
    "        \n",
    "\n",
    "    def forward(self, batch):\n",
    "        # there are some choices, as to how you can define the input to the forward function I prefer it this\n",
    "        # way, where the batch contains the input_ids, the input_put_mask and sometimes the labels (for\n",
    "        # training)\n",
    "        \n",
    "        #print(batch['input_ids'].shape)\n",
    "        #print(batch['labels'].shape)\n",
    "                \n",
    "        outputs = self.model(input_ids=batch['input_ids'],\n",
    "                        attention_mask=batch['attention_mask'],\n",
    "                        labels=batch['labels'])\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        return loss\n",
    "    \n",
    "#     def on_train_batch_start(self, batch, batch_idx, dataloader_idx):\n",
    "#         self.t2=time.time()\n",
    "    \n",
    "#     def on_train_epoch_start(self):\n",
    "#         self.t0=time.time()\n",
    "    \n",
    "    \n",
    "#     def on_train_batch_end(self, outputs, batch, batch_idx, dataloader_idx):\n",
    "        \n",
    "#         if batch_idx%50 ==0:\n",
    "#             t1=time.time()        \n",
    "#             #print(\"Batch {0} of {1}: {2:.6f}\".format(batch_idx+1, self.total_train_batches, t1-self.t0))\n",
    "#             print(\"Batch {0}: {1:.4f}\".format(batch_idx, t1-self.t0))\n",
    "    \n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # the training step is a (virtual) method,specified in the interface, that the pl.LightningModule\n",
    "        # class stipulates you to overwrite. This we do here, by virtue of this definition\n",
    "        \n",
    "        # self refers to the model, which in turn acceses the forward method\n",
    "        loss = self(batch)\n",
    "        \n",
    "        #tensorboard_logs = {'train_loss': loss}\n",
    "        # pytorch lightning allows you to use various logging facilities, eg tensorboard with tensorboard we\n",
    "        # can track and easily visualise the progress of training. In this case\n",
    "        \n",
    "        self.log('loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        #self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        #return {'loss': loss, 'log': tensorboard_logs}\n",
    "        # the training_step method expects a dictionary, which should at least contain the loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        val_loss = self(batch)\n",
    "        self.log('val_loss', val_loss, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return val_loss\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        test_loss = self(batch)\n",
    "        self.log('test_loss', test_loss, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return test_loss\n",
    "    \n",
    "#     def on_train_epoch_end(self, train_step_outputs):\n",
    "#         print('Epoch ',self.current_epoch,' done: took ',time.time()-self.t0, ' sec')\n",
    "\n",
    "        #---------------------        \n",
    "#         print(train_step_outputs)        \n",
    "#         import pdb; pdb.set_trace()\n",
    "\n",
    "#         avg_loss = torch.stack([x['loss'] for x in train_step_outputs]).mean()\n",
    "    \n",
    "#         print(torch.stack([x['loss'] for x in train_step_outputs]))\n",
    "#         tensorboard_logs = {'train_loss': avg_loss}\n",
    "        \n",
    "#         self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "#         self.log('log', tensorboard_logs, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "#         return {\n",
    "#             'train_loss': avg_loss,\n",
    "#             'log': tensorboard_logs,\n",
    "#             'progress_bar': {\n",
    "#                 'train_loss': avg_loss\n",
    "#             }\n",
    "#         }\n",
    "#---------------------\n",
    "\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        # The configure_optimizers is a (virtual) method, specified in the interface, that the\n",
    "        # pl.LightningModule class wants you to overwrite.\n",
    "        # In this case we define that some parameters are optimized in a different way than others. In\n",
    "        # particular we single out parameters that have 'bias', 'LayerNorm.weight' in their names. For those\n",
    "        # we do not use an optimization technique called weight decay.\n",
    "\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "        optimizer_grouped_parameters = [{\n",
    "            'params': [\n",
    "                p for n, p in self.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            'weight_decay':\n",
    "            0.01\n",
    "        }, {\n",
    "            'params': [\n",
    "                p for n, p in self.named_parameters()\n",
    "                if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            'weight_decay':\n",
    "            0.0\n",
    "        }]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                          lr=self.hparams.learning_rate,\n",
    "                          eps=1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                          )\n",
    "\n",
    "        \n",
    "        # We also use a scheduler that is supplied by transformers.\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=0, # Default value in run_glue.py\n",
    "            num_training_steps=self.hparams.num_training_steps)\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "#---------------------\n",
    "#     def freeze(self) -> None:\n",
    "#         # freeze all layers, except the final classifier layers\n",
    "#         for name, param in self.model.named_parameters():\n",
    "#             if 'classifier' not in name:  # classifier layer\n",
    "#                 param.requires_grad = False\n",
    "\n",
    "\n",
    "#         self._frozen = True\n",
    "\n",
    "#     def unfreeze(self) -> None:\n",
    "#         if self._frozen:\n",
    "#             for name, param in self.model.named_parameters():\n",
    "#                 if 'classifier' not in name:  # classifier layer\n",
    "#                     param.requires_grad = True\n",
    "\n",
    "#         self._frozen = False\n",
    "\n",
    "#     def on_epoch_start(self):\n",
    "#         \"\"\"pytorch lightning hook\"\"\"\n",
    "#         if self.current_epoch < self.hparams.nr_frozen_epochs:\n",
    "#             self.freeze()\n",
    "\n",
    "#         if self.current_epoch >= self.hparams.nr_frozen_epochs:\n",
    "#             self.unfreeze()\n",
    "#---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data class definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we finally arrive at the definition of our data class derived from pl.LightningDataModule.\n",
    "\n",
    "In earlier versions of pytorch lightning  (prior to 0.9) the methods here were part of the model class\n",
    "derived from pl.LightningModule. For better flexibility and readability the Data and Model related parts\n",
    "were split out into two different classes:\n",
    "\n",
    "pl.LightningDataModule and pl.LightningModule\n",
    "\n",
    "with the Model related part remaining in pl.LightningModule\n",
    "\n",
    "This is explained in more detail in this video: https://www.youtube.com/watch?v=L---MBeSXFw\n",
    "```\n",
    "class CDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "```\n",
    "\n",
    "\n",
    "Another testing code\n",
    "\n",
    "```\n",
    "class CDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, collator, labels):\n",
    "        self.encodings = encodings\n",
    "        self.collator = collator\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}        \n",
    "        item = self.collator([item])\n",
    "        item = {key: val[0] for key, val in item.items()}\n",
    "    \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "A=AutoTokenizer\n",
    "berttokenizer=A.from_pretrained('bert-base-uncased')\n",
    "datacollator=DataCollatorForLanguageModeling(tokenizer=berttokenizer,mlm_probability=0.15, mlm=True)\n",
    "\n",
    "data, sentences, labels = getDummyDataset()\n",
    "\n",
    "train_encodings = berttokenizer(sentences, truncation=True, padding=True)\n",
    "\n",
    "dataset = CDataset(train_encodings,datacollator)\n",
    "\n",
    "dataset[0]\n",
    "```\n",
    "\n",
    "cf this open issue: https://github.com/PyTorchLightning/pytorch-lightning/issues/3232"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels, collator):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.collator = collator\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}            \n",
    "        item = self.collator([item])\n",
    "        item = {key: val[0] for key, val in item.items()}\n",
    "    \n",
    "        if type(self.labels)!=torch.Tensor:\n",
    "            item['org_labels']= torch.tensor(self.labels[idx])\n",
    "        else:\n",
    "            item['org_labels']= self.labels[idx]\n",
    "    \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "class DataProcessing(pl.LightningDataModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "#         self.save_hyperparameters()\n",
    "        if isinstance(args, tuple): args = args[0]\n",
    "        self.hparams = args\n",
    "        self.batch_size=self.hparams.batch_size        \n",
    "\n",
    "#         print('args:', args)\n",
    "#         print('kwargs:', kwargs)\n",
    "#         print(f'self.hparams.pretrained:{self.hparams.pretrained}')\n",
    "\n",
    "        #print('Loading BERT tokenizer')\n",
    "        print(f'PRETRAINED:{self.hparams.pretrained}')\n",
    "\n",
    "        A = AutoTokenizer\n",
    "        self.tokenizer = A.from_pretrained(self.hparams.pretrained, use_fast=True)\n",
    "\n",
    "        print('Tokenizer:', type(self.tokenizer))\n",
    "        \n",
    "        self.datacollator=DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm_probability=0.15)\n",
    "\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        \n",
    "        MAX_TEXT_LENGTH=512\n",
    "    \n",
    "#         CVE dataset\n",
    "#         ------------------------------------        \n",
    "        data, df_CVE, df_CWE=None,None,None\n",
    "    \n",
    "        if self.hparams.rand_dataset=='dummy':            \n",
    "            #------------------------------------\n",
    "            data, sentences, labels = getDummyDataset()        \n",
    "            #------------------------------------\n",
    "        else:        \n",
    "            if self.hparams.rand_dataset=='temporal':\n",
    "                print(\"Temporal Partition:--\")\n",
    "                data, df_CVE, df_CWE = getDataset(DATASET_LOAD_DIR)            \n",
    "            else:\n",
    "                print(\"Random Partition:--\")\n",
    "                data, df_CVE, df_CWE = getRandomDataset(DATASET_LOAD_DIR, 0.70, 0.10)\n",
    "\n",
    "            sentences=df_CVE['CVE Description'].apply(lambda x: x[:MAX_TEXT_LENGTH])\n",
    "            labels=data.y\n",
    "            CWE_IDS_USED=df_CWE['Name'].tolist()\n",
    "            INDEX_TO_CWE_MAP=dict(zip(list(range(len(CWE_IDS_USED))),CWE_IDS_USED))\n",
    "            CWE_TO_INDEX_MAP=dict(zip(CWE_IDS_USED,list(range(len(CWE_IDS_USED)))))\n",
    "            sentences=sentences.tolist()\n",
    "        \n",
    "        \n",
    "        if type(labels)!=torch.Tensor:\n",
    "            labels=torch.tensor(labels,dtype=torch.long)\n",
    "        else:\n",
    "            labels=labels.type(torch.LongTensor)\n",
    "        \n",
    "        self.NUM_CLASSES=len(data.y[0])\n",
    "    \n",
    "        train_encodings = self.tokenizer(sentences, truncation=True, padding=True, max_length=MAX_TEXT_LENGTH)        \n",
    "        self.dataset = CDataset(train_encodings, labels, self.datacollator)        \n",
    "        \n",
    "        val_mask= (data.val_mask == True).nonzero().flatten().numpy()\n",
    "        val_encodings = self.tokenizer([sentences[i] for i in val_mask], truncation=True, padding=True, max_length=MAX_TEXT_LENGTH)\n",
    "        self.val_dataset=CDataset(val_encodings, labels[data.val_mask], self.datacollator)\n",
    "        \n",
    "        test_mask= (data.test_mask == True).nonzero().flatten().numpy()\n",
    "        test_encodings = self.tokenizer([sentences[i] for i in test_mask], truncation=True, padding=True, max_length=MAX_TEXT_LENGTH)\n",
    "        self.test_dataset=CDataset(test_encodings, labels[data.test_mask], self.datacollator)\n",
    "                \n",
    "        #print('Example Sentence[0]: ', sentences[0])              \n",
    "    \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        \n",
    "        train_sampler = RandomSampler(self.dataset)\n",
    "        \n",
    "        return DataLoader(self.dataset,\n",
    "                         #sampler=train_sampler, \n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=min(NUM_PROCESSORS,self.batch_size,0)\n",
    "                         )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        \n",
    "        val_sampler = SequentialSampler(self.val_dataset)\n",
    "        \n",
    "        return DataLoader(self.val_dataset,\n",
    "                          sampler=val_sampler, \n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=min(NUM_PROCESSORS,self.batch_size,0)\n",
    "                         )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        \n",
    "        test_sampler = SequentialSampler(self.test_dataset)\n",
    "        \n",
    "        return DataLoader(self.test_dataset,\n",
    "                          sampler=test_sampler, \n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=min(NUM_PROCESSORS,self.batch_size,0)\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printModelParams(model):\n",
    "    print (model)\n",
    "    # Get all of the model's parameters as a list of tuples.\n",
    "    params = list(model.named_parameters())\n",
    "    print('The model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "    print('==== Embedding Layer ====\\n')\n",
    "    for p in params[0:5]:\n",
    "        print(\"{:<55} {:>12}, {}\".format(p[0], str(tuple(p[1].size())),p[1].requires_grad))\n",
    "\n",
    "    print('\\n==== First Transformer ====\\n')\n",
    "    for p in params[5:21]:\n",
    "        print(\"{:<55} {:>12}, {}\".format(p[0], str(tuple(p[1].size())),p[1].requires_grad))\n",
    "\n",
    "    print('\\n==== Output Layer ====\\n')\n",
    "    for p in params[-5:]:\n",
    "        print(\"{:<55} {:>12}, {}\".format(p[0], str(tuple(p[1].size())),p[1].requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_value(model):\n",
    "    params = list(model.named_parameters())\n",
    "    print (params[-1][0],params[-1][1][:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main\n",
    "Two key aspects:\n",
    "\n",
    "- pytorch lightning can add arguments to the parser automatically\n",
    "- you can manually add your own specific arguments.\n",
    "\n",
    "- there is a little more code than seems necessary, because of a particular argument the scheduler\n",
    "  needs. There is currently an open issue on this complication\n",
    "  https://github.com/PyTorchLightning/pytorch-lightning/issues/1038"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Batching (not used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "if args.auto_batch>0:    \n",
    "        #init_batch_size=32\n",
    "        init_batch_size=args.auto_batch\n",
    "        tuner = Tuner(trainer)        \n",
    "        assert hasattr(dataProcessor, \"batch_size\")\n",
    "        new_batch_size = tuner.scale_batch_size(model, \n",
    "                                                mode=\"binsearch\", \n",
    "                                                init_val=init_batch_size, \n",
    "                                                max_trials=10,\n",
    "                                                datamodule=dataProcessor,                                        \n",
    "                                               )\n",
    "\n",
    "        print(\"Max batch size: \", new_batch_size)\n",
    "        #dataProcessor.batch_size = new_batch_size\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Configuration to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "def get_configuration():\n",
    "    parser = ArgumentParser()\n",
    "\n",
    "    #parser.add_argument('--pretrained', type=str, default=\"bert-base-uncased\")\n",
    "    #parser.add_argument('--pretrained', type=str, default=\"roberta-base\")\n",
    "    parser.add_argument('--pretrained', type=str, default=\"distilbert-base-uncased\") \n",
    "    parser.add_argument('--epochs', type=int, default=2)\n",
    "    parser.add_argument('--nr_frozen_epochs', type=int, default=5)\n",
    "    parser.add_argument('--training_portion', type=float, default=0.9)\n",
    "    parser.add_argument('--batch_size', type=int, default=32)\n",
    "    parser.add_argument('--auto_batch', type=int, default=-1)\n",
    "    parser.add_argument('--learning_rate', type=float, default=2e-5)\n",
    "    parser.add_argument('--frac', type=float, default=1)\n",
    "    parser.add_argument('--num_gpus', type=int, default=-1)\n",
    "    parser.add_argument('--nodes', type=int, default=1)\n",
    "    parser.add_argument('--parallel_mode', type=str, default=\"dp\", choices=['dp', 'ddp', 'ddp2'])\n",
    "    parser.add_argument('--refresh_rate', type=int, default=1)\n",
    "    parser.add_argument('--check', type=bool, default=True)\n",
    "    parser.add_argument('--rand_dataset', type=str, default=\"dummy\", choices=['temporal','random','dummy'])\n",
    "    \n",
    "    \n",
    "    parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "\n",
    "    # parser = Model.add_model_specific_args(parser) parser = Data.add_model_specific_args(parser)\n",
    "    parser = pl.Trainer.add_argparse_args(parser)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print(\"-\"*50)\n",
    "    print(\"BATCH SIZE: \", args.batch_size)\n",
    "    \n",
    "    # start : get training steps\n",
    "    dataProcessor = DataProcessing(args)\n",
    "    dataProcessor.setup()\n",
    "    \n",
    "    args.num_training_steps = len(dataProcessor.train_dataloader())*args.epochs\n",
    "    dict_args = vars(args)\n",
    "    \n",
    "    gpus=-1\n",
    "    if NUM_GPUS>0:\n",
    "        gpus=args.num_gpus        \n",
    "    else:\n",
    "        args.parallel_mode=None\n",
    "        gpus=None\n",
    "    \n",
    "    print(\"USING GPUS:\", gpus)\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # saves a file like: my/path/sample-mnist-epoch=02-val_loss=0.32.ckpt\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='loss_epoch',\n",
    "        dirpath=MODEL_SAVE_DIR,\n",
    "        #filename='{epoch:02d}-{loss:.4f}',\n",
    "        filename=\"CBERT-\"+args.pretrained, #+'-'+str(args.parallel_mode)\n",
    "        save_top_k=1,\n",
    "        mode='min',\n",
    "        save_weights_only=True,\n",
    "        #prefix=\"CBERT-\"+args.pretrained,#+'-'+str(args.parallel_mode),\n",
    "        save_last=True,\n",
    "    )\n",
    "    \n",
    "#     if args.check==False:\n",
    "#         args.checkpoint_callback = False\n",
    "#     elif args.parallel_mode=='dp':\n",
    "#         args.callbacks=[checkpoint_callback]        \n",
    "#     else:\n",
    "#         args.checkpoint_callback = False\n",
    "\n",
    "    args.checkpoint_callback = False\n",
    "    \n",
    "    trainer = pl.Trainer.from_argparse_args(args, \n",
    "                                            gpus=gpus,\n",
    "                                            num_nodes=args.nodes, \n",
    "                                            accelerator=args.parallel_mode,\n",
    "                                            max_epochs=args.epochs, \n",
    "                                            gradient_clip_val=1.0,                                            \n",
    "                                            #logger=False,\n",
    "                                            logger=TensorBoardLogger(\"tb_logs\", name=\"pretraining\"),\n",
    "                                            progress_bar_refresh_rate=args.refresh_rate,\n",
    "                                            profiler='simple', #'simple',\n",
    "                                            default_root_dir=MODEL_SAVE_DIR,                                            \n",
    "                                            deterministic=True,\n",
    "                                           )\n",
    "\n",
    "    return trainer, dataProcessor, args, dict_args\n",
    "\n",
    "# trainer, dataProcessor, args, dict_args = get_configuration()\n",
    "# next(iter(dataProcessor.test_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():    \n",
    "    trainer, dataProcessor, args, dict_args = get_configuration()\n",
    "    \n",
    "    model = Model(**dict_args)    \n",
    "    \n",
    "#     printModelParams(model)\n",
    "#     args.early_stop_callback = EarlyStopping('val_loss')\n",
    "    \n",
    "    \n",
    "    print(\"Original weights: \");print_model_value(model)\n",
    "    \n",
    "    t0=time.time()\n",
    "    trainer.fit(model, dataProcessor)\n",
    "    print('Training took: ',time.time()-t0)\n",
    "    \n",
    "    print(\"Trained weights: \");print_model_value(model)\n",
    "    \n",
    "#     #if args.parallel_mode!='dp':    \n",
    "#     print(\"Saving the last model\")\n",
    "#     #MODEL_NAME=MODEL_SAVE_DIR+\"CBERT-\"+args.pretrained+'-'+args.parallel_mode+\".ckpt\"\n",
    "#     MODEL_NAME=MODEL_SAVE_DIR+\"CBERT-\"+args.pretrained+\".ckpt\"\n",
    "#     trainer.save_checkpoint(MODEL_NAME)\n",
    "\n",
    "#     print(\"Testing:....\")\n",
    "#     trainer.test(model, dataProcessor.test_dataloader())\n",
    "    \n",
    "    print(\"Training Phase Complete......\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    trainer, dataProcessor, args, dict_args = get_configuration()\n",
    "    \n",
    "    #MODEL_NAME=MODEL_SAVE_DIR+\"CBERT-\"+args.pretrained+'-'+args.parallel_mode+\".ckpt\"    \n",
    "    MODEL_NAME=MODEL_SAVE_DIR+\"CBERT-\"+args.pretrained+\".ckpt\"    \n",
    "#     if args.parallel_mode=='dp':\n",
    "#         MODEL_NAME=MODEL_SAVE_DIR+\"CBERT-\"+args.pretrained+'-'+args.parallel_mode+\"-last.ckpt\"\n",
    "    \n",
    "    if os.path.exists(MODEL_NAME): \n",
    "        print('Loading Saved Model: ',MODEL_NAME)        \n",
    "    else: \n",
    "        print(\"File not found: \",MODEL_NAME)\n",
    "        return\n",
    "    \n",
    "    model=None\n",
    "    \n",
    "    if args.parallel_mode!='dp':\n",
    "        model = Model.load_from_checkpoint(MODEL_NAME)\n",
    "    else:\n",
    "        model = Model(**dict_args)\n",
    "        print(\"Original weights: \");print_model_value(model)\n",
    "        checkpoint = torch.load(MODEL_NAME, map_location=lambda storage, loc: storage)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    print(\"Loaded weights: \");print_model_value(model)    \n",
    "    trainer.test(model, dataProcessor.test_dataloader())    \n",
    "    print(\"Test Complete......\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "BATCH SIZE:  32\n",
      "PRETRAINED:distilbert-base-uncased\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-c08a2f800ee7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m#test_model()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-d04b49f98ead>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataProcessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_configuration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdict_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-44c70b46ec9f>\u001b[0m in \u001b[0;36mget_configuration\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# start : get training steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mdataProcessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataProcessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mdataProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-8b7ab24a4e23>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tokenizer:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py38cu11/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0mtokenizer_class_py\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTOKENIZER_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_fast\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py38cu11/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1683\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                     resolved_vocab_files[file_id] = cached_path(\n\u001b[0m\u001b[1;32m   1686\u001b[0m                         \u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                         \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py38cu11/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1361\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_remote_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m         \u001b[0;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m         output_path = get_from_cache(\n\u001b[0m\u001b[1;32m   1364\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cent7/2020.11-py38/py38cu11/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1584\u001b[0m                     )\n\u001b[1;32m   1585\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1586\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m   1587\u001b[0m                         \u001b[0;34m\"Connection error, and we cannot find the requested files in the cached path.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m                         \u001b[0;34m\" Please try again or make sure your Internet connection is on.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_model()\n",
    "    #test_model()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
