{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: Tesla P100-PCIE-16GB\n",
      "cuda\n",
      "Cpu count:  20\n"
     ]
    }
   ],
   "source": [
    "import torch# If there's a GPU available...\n",
    "import random\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" ##I will find a way to fix this later :(\n",
    "\n",
    "NUM_GPUS=0\n",
    "\n",
    "try:\n",
    "    if torch.cuda.is_available():  \n",
    "        device = torch.device(\"cuda\")\n",
    "        NUM_GPUS=torch.cuda.device_count()\n",
    "        print('There are %d GPU(s) available.' % NUM_GPUS)\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name())# If not...\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")  \n",
    "except:\n",
    "    print('Cuda error using CPU instead.')\n",
    "    device = torch.device(\"cpu\")  \n",
    "    \n",
    "print(device)\n",
    "\n",
    "NUM_PROCESSORS=multiprocessing.cpu_count()\n",
    "print(\"Cpu count: \",NUM_PROCESSORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading directory:  ./Results\n",
      "Model Saving directory: ./Results/NVD/Model/\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.Dataset import getDataset, getDummyDataset, getRandomDataset, Data        \n",
    "\n",
    "DIR='./Results'\n",
    "    \n",
    "from pathlib import Path\n",
    "Path(DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATASET_LOAD_DIR=\"./NVD/\"\n",
    "MODEL_SAVE_DIR=DIR+'/NVD/Model/'\n",
    "\n",
    "Path(MODEL_SAVE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "print(\"Data loading directory: \", DIR)\n",
    "print(\"Model Saving directory:\", MODEL_SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some more packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "import zipfile\n",
    "import wget\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import RandomSampler,SequentialSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss, CosineEmbeddingLoss\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, AutoModelWithLMHead\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers.models.bert.modeling_bert import BertOnlyMLMHead\n",
    "from transformers.models.roberta.modeling_roberta import RobertaLMHead\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.tuner.tuning import Tuner\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.trainer.supporters import CombinedLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For reproduciblity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Set the seed value all over the place to make this reproducible.\n",
    "from random import sample\n",
    "\n",
    "seed_val = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_val)\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "pl.seed_everything(seed_val)\n",
    "\n",
    "try:\n",
    "    torch.cuda.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "except:\n",
    "    print(\"nothing to set for cudnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data class definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy precision level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_NEG_LINKS=None\n",
    "TOP_K0=[1,3,5]\n",
    "TOP_K1=[1,2,2]\n",
    "TOP_K2=[1,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_results(filename,infos,append=True):    \n",
    "    f=None\n",
    "    if append==True:  \n",
    "        f=open(filename, 'a+')\n",
    "    else:\n",
    "        f=open(filename, 'w')\n",
    "    \n",
    "    f.write(\"\\n\")    \n",
    "    for key, values in infos.items():\n",
    "        f.write(\"%s :\" % key)\n",
    "        if type(values).__name__=='list':            \n",
    "            for item in values:\n",
    "                f.write(\"%s \" % item)\n",
    "        else:\n",
    "            f.write(\"%s \" % values)           \n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    f.close()\n",
    "#log_results(\"testing\",{'train_acc':[1,2,3,4]})\n",
    "#log_results(\"testing\",{'train_acc':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CVE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels, org_labels, collator, data, k_neg_link=10, use_collator=True):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.org_labels = org_labels\n",
    "        self.collator = collator\n",
    "        self.data = data\n",
    "        self.k_neg_link=k_neg_link\n",
    "        self.use_collator=use_collator\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}            \n",
    "        if self.use_collator:\n",
    "            item = self.collator([item])\n",
    "            item = {key: val[0] for key, val in item.items()}\n",
    "        item['h_labels']= self.labels[idx]\n",
    "        item['o_labels']= self.org_labels[idx]\n",
    "        \n",
    "        POS=[]\n",
    "        NEG=[]\n",
    "        \n",
    "        #------POSITIVE LINKS-----\n",
    "        p_labels=(self.labels[idx]==1).nonzero().flatten()        \n",
    "        for c in p_labels: POS.append(c.item())\n",
    "        \n",
    "        #------NEG LINKS-----\n",
    "        n_labels=(self.labels[idx]==0).nonzero().flatten()    \n",
    "        limit=min(len(n_labels),self.k_neg_link)\n",
    "        indexs=np.random.choice(len(n_labels),limit, replace=False)\n",
    "        n_labels=n_labels[indexs]            \n",
    "        for c in n_labels: NEG.append(c.item())\n",
    "        \n",
    "        NEG=(NEG*int(self.k_neg_link/len(NEG)+1))[:self.k_neg_link]\n",
    "        POS=(POS*int(self.k_neg_link/len(POS)+1))[:self.k_neg_link]\n",
    "        \n",
    "        item['pos']=torch.tensor(POS, dtype=torch.long)\n",
    "        item['neg']=torch.tensor(NEG, dtype=torch.long)\n",
    "        \n",
    "        item['pos_label']=torch.ones(len(POS), dtype=torch.long)\n",
    "        item['neg_label']=torch.zeros(len(NEG), dtype=torch.long)\n",
    "             \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing CVE dataset\n",
    "```\n",
    "A=AutoTokenizer\n",
    "berttokenizer=A.from_pretrained('bert-base-uncased')\n",
    "datacollator=DataCollatorForLanguageModeling(tokenizer=berttokenizer,mlm_probability=0.15, mlm=True)\n",
    "\n",
    "data, sentences, labels = getDummyDataset()\n",
    "if type(labels)!=torch.Tensor:\n",
    "    labels=torch.tensor(labels,dtype=torch.long)\n",
    "else:\n",
    "    labels=labels.type(torch.LongTensor)\n",
    "    \n",
    "train_encodings = berttokenizer(sentences, truncation=True, padding=True)\n",
    "dataset = CDataset(train_encodings, labels, labels, datacollator, data)\n",
    "\n",
    "dataset[1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CWE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels, org_labels, collator, data, use_collator=True):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.org_labels = org_labels\n",
    "        self.collator = collator\n",
    "        self.data = data\n",
    "        self.use_collator=use_collator\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}            \n",
    "        if self.use_collator:\n",
    "            item = self.collator([item])\n",
    "            item = {key: val[0] for key, val in item.items()}\n",
    "        item['h_labels']= self.labels[idx]\n",
    "        item['o_labels']= self.org_labels[idx]\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing CWE Dataset\n",
    "```\n",
    "A=AutoTokenizer\n",
    "berttokenizer=A.from_pretrained('bert-base-uncased')\n",
    "datacollator=DataCollatorForLanguageModeling(tokenizer=berttokenizer,mlm_probability=0.15, mlm=True)\n",
    "\n",
    "data, sentences, labels = getDummyDataset()\n",
    "if type(labels)!=torch.Tensor:\n",
    "    labels=torch.tensor(labels,dtype=torch.long)\n",
    "else:\n",
    "    labels=labels.type(torch.LongTensor)\n",
    "    \n",
    "    \n",
    "class_mask=(data.class_mask == True).nonzero().flatten().numpy()\n",
    "class_encodings = berttokenizer([sentences[i] for i in class_mask], truncation=True, padding=True)\n",
    "\n",
    "dataset = CDataset(class_encodings, labels, labels, datacollator, data)\n",
    "\n",
    "dataset[3]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessing(pl.LightningDataModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.save_hyperparameters()\n",
    "        if isinstance(args, tuple): args = args[0]\n",
    "        self.hparams = args\n",
    "        self.batch_size=self.hparams.batch_size        \n",
    "\n",
    "        print(f'PRETRAINED:{self.hparams.pretrained}')\n",
    "\n",
    "        A = AutoTokenizer\n",
    "        self.tokenizer = A.from_pretrained(self.hparams.pretrained, use_fast=True)\n",
    "        print('Tokenizer:', type(self.tokenizer))\n",
    "        \n",
    "        self.datacollator=DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm_probability=0.15)\n",
    "    \n",
    "    \n",
    "    def get_cwe_level(self):\n",
    "        self.levels={}\n",
    "        for key,values in self.data.depth.items():\n",
    "            if type(values)==int:\n",
    "                values=[values]\n",
    "\n",
    "            for val in values:\n",
    "                if val in self.levels:\n",
    "                    if(key not in self.levels[val]):self.levels[val].append(key)\n",
    "                else:\n",
    "                    self.levels[val]=[key]\n",
    "\n",
    "#         for i,j in self.levels.items():\n",
    "#             print(i,\"->\",len(j),':',j)\n",
    "            \n",
    "    \n",
    "    def indexsToUpdate(self,parentid,indexs):\n",
    "        if parentid==-1:\n",
    "            return\n",
    "\n",
    "        if(parentid not in indexs):\n",
    "            indexs.append(parentid)\n",
    "\n",
    "        parents=self.data.child_parent[parentid]\n",
    "\n",
    "        for parent in parents:\n",
    "                self.indexsToUpdate(parent,indexs)\n",
    "\n",
    "        return\n",
    "    \n",
    "    def updates_label_by_hierarchy(self,labels):\n",
    "        all_mask=self.data.train_mask|self.data.val_mask|self.data.test_mask|self.data.class_mask\n",
    "        labeled_indexs= (all_mask == True).nonzero().flatten().numpy()\n",
    "\n",
    "        for i in labeled_indexs:    \n",
    "            row_labels=(labels[i]==1).nonzero().flatten().numpy()    \n",
    "\n",
    "            #print(row_labels,\"->\",end='')\n",
    "\n",
    "            indexs=[]\n",
    "            for r_label in row_labels:\n",
    "                parents=self.data.child_parent[r_label]\n",
    "\n",
    "                for parent in parents:\n",
    "                    self.indexsToUpdate(parent,indexs)\n",
    "\n",
    "            #print(indexs)\n",
    "\n",
    "            labels[i][indexs]=1\n",
    "\n",
    "        return labels\n",
    "               \n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        MAX_TEXT_LENGTH=512\n",
    "    \n",
    "#         CVE dataset\n",
    "#         ------------------------------------        \n",
    "        data, df_CVE, df_CWE=None,None,None\n",
    "    \n",
    "        if self.hparams.rand_dataset=='dummy':            \n",
    "            #------------------------------------\n",
    "            data, sentences, labels = getDummyDataset()        \n",
    "            #------------------------------------\n",
    "        else:        \n",
    "            if self.hparams.rand_dataset=='temporal':\n",
    "                print(\"Temporal Partition:--\")\n",
    "                data, df_CVE, df_CWE = getDataset(DATASET_LOAD_DIR)            \n",
    "            else:\n",
    "                print(\"Random Partition:--\")\n",
    "                data, df_CVE, df_CWE = getRandomDataset(DATASET_LOAD_DIR, 0.70, 0.10)\n",
    "\n",
    "            sentences=df_CVE['CVE Description'].apply(lambda x: x[:MAX_TEXT_LENGTH])\n",
    "            labels=data.y\n",
    "            CWE_IDS_USED=df_CWE['Name'].tolist()\n",
    "            INDEX_TO_CWE_MAP=dict(zip(list(range(len(CWE_IDS_USED))),CWE_IDS_USED))\n",
    "            CWE_TO_INDEX_MAP=dict(zip(CWE_IDS_USED,list(range(len(CWE_IDS_USED)))))\n",
    "            sentences=sentences.tolist()\n",
    "        \n",
    "#         #------------------------------------        \n",
    "#         #CVE dummy dataset\n",
    "#         #------------------------------------\n",
    "#         data, sentences, labels = getDummyDataset()        \n",
    "#         #------------------------------------\n",
    "\n",
    "        \n",
    "        \n",
    "        if type(labels)!=torch.Tensor:\n",
    "            labels=torch.tensor(labels,dtype=torch.long)\n",
    "        else:\n",
    "            labels=labels.type(torch.LongTensor)\n",
    "        self.data=data        \n",
    "        org_labels=np.copy(labels) ###keep the copy of orgiginal labels\n",
    "        \n",
    "        self.get_cwe_level()        \n",
    "        labels=self.updates_label_by_hierarchy(labels)\n",
    "\n",
    "        \n",
    "        self.NUM_CLASSES=len(data.y[0])\n",
    "        \n",
    "        train_mask= (data.train_mask == True).nonzero().flatten().numpy()\n",
    "        val_mask= (data.val_mask == True).nonzero().flatten().numpy()\n",
    "        test_mask= (data.test_mask == True).nonzero().flatten().numpy()\n",
    "        class_mask=(data.class_mask == True).nonzero().flatten().numpy()\n",
    "        \n",
    "        print(\"Train size:\", len(train_mask))\n",
    "        print(\"Val size:\", len(val_mask))\n",
    "        print(\"Test size:\", len(test_mask))\n",
    "        print(\"Class size:\",len(class_mask))\n",
    "\n",
    "        \n",
    "        train_encodings = self.tokenizer([sentences[i] for i in train_mask], truncation=True, padding=True, max_length=MAX_TEXT_LENGTH)        \n",
    "        \n",
    "        self.dataset = CDataset(\n",
    "            train_encodings, \n",
    "            labels[data.train_mask], \n",
    "            org_labels[data.train_mask],\n",
    "            self.datacollator,\n",
    "            data,\n",
    "            K_NEG_LINKS\n",
    "        )\n",
    "        self.datasetNC = CDataset(\n",
    "            train_encodings, \n",
    "            labels[data.train_mask], \n",
    "            org_labels[data.train_mask],\n",
    "            self.datacollator,\n",
    "            data,\n",
    "            K_NEG_LINKS,\n",
    "            use_collator=False\n",
    "        )        \n",
    "                \n",
    "        val_encodings = self.tokenizer([sentences[i] for i in val_mask], truncation=True, padding=True, max_length=MAX_TEXT_LENGTH)\n",
    "        self.val_dataset=CDataset(\n",
    "            val_encodings, \n",
    "            labels[data.val_mask],\n",
    "            org_labels[data.val_mask],\n",
    "            self.datacollator,\n",
    "            data,\n",
    "            K_NEG_LINKS\n",
    "        )\n",
    "        self.val_datasetNC=CDataset(\n",
    "            val_encodings, \n",
    "            labels[data.val_mask],\n",
    "            org_labels[data.val_mask],\n",
    "            self.datacollator,\n",
    "            data,\n",
    "            K_NEG_LINKS,\n",
    "            use_collator=False\n",
    "        )\n",
    "        \n",
    "        test_encodings = self.tokenizer([sentences[i] for i in test_mask], truncation=True, padding=True, max_length=MAX_TEXT_LENGTH)\n",
    "        self.test_dataset=CDataset(\n",
    "            test_encodings,\n",
    "            labels[data.test_mask],\n",
    "            org_labels[data.test_mask],\n",
    "            self.datacollator,\n",
    "            data,\n",
    "            K_NEG_LINKS\n",
    "        )\n",
    "        self.test_datasetNC=CDataset(\n",
    "            test_encodings,\n",
    "            labels[data.test_mask],\n",
    "            org_labels[data.test_mask],\n",
    "            self.datacollator,\n",
    "            data,\n",
    "            K_NEG_LINKS,\n",
    "            use_collator=False\n",
    "        )\n",
    "        \n",
    "        class_encodings = self.tokenizer([sentences[i] for i in class_mask], truncation=True, padding=True, max_length=MAX_TEXT_LENGTH)        \n",
    "        self.class_dataset = DDataset(\n",
    "            class_encodings, \n",
    "            labels[data.class_mask], \n",
    "            org_labels[data.class_mask],\n",
    "            self.datacollator,\n",
    "            data\n",
    "        )\n",
    "        self.class_datasetNC = DDataset(\n",
    "            class_encodings, \n",
    "            labels[data.class_mask], \n",
    "            org_labels[data.class_mask],\n",
    "            self.datacollator,\n",
    "            data,\n",
    "            use_collator=False\n",
    "        )\n",
    "    \n",
    "    def class_dataloader(self, use_collator=True):\n",
    "        dataset=None\n",
    "        if use_collator:\n",
    "            dataset=self.class_dataset\n",
    "        else:\n",
    "            dataset=self.class_datasetNC\n",
    "            \n",
    "        class_sampler = SequentialSampler(dataset)\n",
    "        \n",
    "        loader_cwe = DataLoader(\n",
    "            dataset,\n",
    "            sampler=class_sampler, \n",
    "            batch_size=len(dataset),\n",
    "            pin_memory=True,\n",
    "            #num_workers=min(NUM_PROCESSORS,self.batch_size, NUM_GPUS*4)\n",
    "        )\n",
    "        \n",
    "        return loader_cwe\n",
    "    \n",
    "    def train_dataloader(self,use_collator=True):\n",
    "        \n",
    "        dataset=None\n",
    "        if use_collator:\n",
    "            dataset=self.dataset\n",
    "        else:\n",
    "            dataset=self.datasetNC\n",
    "        \n",
    "        train_sampler = RandomSampler(dataset)        \n",
    "        loader_cve = DataLoader(\n",
    "            dataset,\n",
    "            sampler=train_sampler, \n",
    "            batch_size=self.batch_size,\n",
    "            pin_memory=True,\n",
    "            #num_workers=min(NUM_PROCESSORS,self.batch_size,NUM_GPUS*4)\n",
    "        )\n",
    "        \n",
    "        return loader_cve\n",
    "    \n",
    "    def val_dataloader(self, use_collator=True):\n",
    "        dataset=None\n",
    "        \n",
    "        if use_collator:\n",
    "            dataset=self.val_dataset\n",
    "        else:\n",
    "            dataset=self.val_datasetNC\n",
    "    \n",
    "        val_sampler = SequentialSampler(dataset)\n",
    "        \n",
    "        loader_cve=DataLoader(\n",
    "            dataset,\n",
    "            sampler=val_sampler, \n",
    "            batch_size=self.batch_size,\n",
    "            pin_memory=True,\n",
    "            #num_workers=min(NUM_PROCESSORS,self.batch_size,NUM_GPUS*4)\n",
    "        )\n",
    "        \n",
    "        return loader_cve\n",
    "    \n",
    "    def test_dataloader(self,use_collator=True):\n",
    "        dataset=None        \n",
    "        if use_collator:\n",
    "            dataset=self.test_dataset\n",
    "        else:\n",
    "            dataset=self.test_datasetNC\n",
    "\n",
    "        test_sampler = SequentialSampler(dataset)        \n",
    "        loader_cve=DataLoader(\n",
    "            dataset,\n",
    "            sampler=test_sampler, \n",
    "            batch_size=self.batch_size,\n",
    "            pin_memory=True,\n",
    "            #num_workers=min(NUM_PROCESSORS,self.batch_size, NUM_GPUS*4)\n",
    "        )\n",
    "        \n",
    "        return loader_cve\n",
    "\n",
    "\n",
    "# #----------------------------\n",
    "# import argparse\n",
    "# from argparse import ArgumentParser\n",
    "\n",
    "# parser = ArgumentParser()\n",
    "# parser.add_argument('--pretrained', type=str, default=\"bert-base-uncased\")\n",
    "# parser.add_argument('--batch_size', type=int, default=32)\n",
    "# parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "# parser = pl.Trainer.add_argparse_args(parser)\n",
    "# args = parser.parse_args()\n",
    "# dataProcessor = DataProcessing(args)\n",
    "# dataProcessor.setup()\n",
    "# #----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Configuration to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "def get_configuration():\n",
    "    parser = ArgumentParser()\n",
    "\n",
    "    #parser.add_argument('--pretrained', type=str, default=\"bert-base-uncased\")\n",
    "    #parser.add_argument('--pretrained', type=str, default=\"roberta-base\")\n",
    "    parser.add_argument('--pretrained', type=str, default=\"distilbert-base-uncased\")    \n",
    "    parser.add_argument('--epochs', type=int, default=25)\n",
    "    parser.add_argument('--nr_frozen_epochs', type=int, default=5)\n",
    "    #parser.add_argument('--training_portion', type=float, default=0.9)\n",
    "    parser.add_argument('--batch_size', type=int, default=32)\n",
    "    parser.add_argument('--auto_batch', type=int, default=-1)\n",
    "    parser.add_argument('--learning_rate', type=float, default=2e-5)\n",
    "    parser.add_argument('--frac', type=float, default=1)\n",
    "    parser.add_argument('--num_gpus', type=int, default=-1)\n",
    "    parser.add_argument('--nodes', type=int, default=1)\n",
    "    parser.add_argument('--parallel_mode', type=str, default=\"dp\", choices=['dp', 'ddp', 'ddp2'])\n",
    "    parser.add_argument('--refresh_rate', type=int, default=100)\n",
    "    parser.add_argument('--checkpointing', type=str, default='True', choices=['True','False'])\n",
    "    parser.add_argument('--use_rd', type=str, default='False', choices=['True','False'])\n",
    "    parser.add_argument('--lm_lambda', type=float, default=0.1)\n",
    "    parser.add_argument('--rand_dataset', type=str, default=\"dummy\", choices=['temporal','random','dummy'])\n",
    "    parser.add_argument('--use_pretrained', type=str, default='True', choices=['True','False'])\n",
    "    parser.add_argument('--neg_link', type=int, default=120)\n",
    "    parser.add_argument('--check', type=bool, default=False)\n",
    "    parser.add_argument('--performance_mode', type=str, default='False', choices=['True','False'])\n",
    "    parser.add_argument('--freeze', type=str, default='True')\n",
    "    \n",
    "    parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "\n",
    "    # parser = Model.add_model_specific_args(parser) parser = Data.add_model_specific_args(parser)\n",
    "    parser = pl.Trainer.add_argparse_args(parser)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    global K_NEG_LINKS\n",
    "    K_NEG_LINKS=args.neg_link    \n",
    "    print(\"-\"*50)\n",
    "    print(\"POS/NEG Links: \",K_NEG_LINKS)\n",
    "    print(\"BATCH SIZE: \", args.batch_size)\n",
    "    \n",
    "    \n",
    "    # start : get training steps\n",
    "    dataProcessor = DataProcessing(args)\n",
    "    dataProcessor.setup()\n",
    "    args.NUM_CLASSES=dataProcessor.NUM_CLASSES\n",
    "    args.MODEL_NAME=\"CBERT-LINK-\"+args.pretrained+'-'+args.parallel_mode\n",
    "    args.MODEL_DIR_FILE=MODEL_SAVE_DIR+args.MODEL_NAME\n",
    "    #args.PRE_TRAINED_MODEL=MODEL_SAVE_DIR+\"CBERT-\"+args.pretrained+'-'+args.parallel_mode+'.ckpt'\n",
    "    args.PRE_TRAINED_MODEL=MODEL_SAVE_DIR+\"CBERT-\"+args.pretrained+'.ckpt'\n",
    "    \n",
    "    args.num_training_steps = len(dataProcessor.train_dataloader())*args.epochs\n",
    "    dict_args = vars(args)\n",
    "    \n",
    "    gpus=-1\n",
    "    if NUM_GPUS>0:\n",
    "        gpus=args.num_gpus\n",
    "    else:\n",
    "        gpus=None\n",
    "        args.parallel_mode=None\n",
    "    \n",
    "    print(\"USING GPUS:\", gpus)\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # saves a file like: my/path/sample-mnist-epoch=02-val_loss=0.32.ckpt\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='loss_epoch',\n",
    "        dirpath=MODEL_SAVE_DIR,\n",
    "        #filename='{epoch:02d}-{loss:.4f}',\n",
    "        filename=\"CBERT-LINK\"+args.pretrained,#+'-'+str(args.parallel_mode),\n",
    "        save_top_k=1,\n",
    "        mode='min',\n",
    "        save_weights_only=True,\n",
    "        #prefix=\"CBERT-LINK\"+args.pretrained+'-'+str(args.parallel_mode),\n",
    "        save_last=True,\n",
    "    )\n",
    "    \n",
    "    if args.check==False:\n",
    "        args.checkpoint_callback = False\n",
    "    elif args.parallel_mode=='dp':\n",
    "        args.callbacks=[checkpoint_callback]        \n",
    "    else:\n",
    "        args.checkpoint_callback = False\n",
    "    \n",
    "    trainer = pl.Trainer.from_argparse_args(args, \n",
    "                                            gpus=gpus,\n",
    "                                            num_nodes=args.nodes, \n",
    "                                            accelerator=args.parallel_mode,\n",
    "                                            max_epochs=args.epochs, \n",
    "                                            gradient_clip_val=1.0,                                            \n",
    "                                            logger=False,\n",
    "                                            progress_bar_refresh_rate=args.refresh_rate,\n",
    "                                            profiler=False, #'simple',\n",
    "                                            default_root_dir=MODEL_SAVE_DIR,                                            \n",
    "                                            deterministic=True,\n",
    "                                           )\n",
    "    \n",
    "    return trainer, dataProcessor, args, dict_args\n",
    "\n",
    "#trainer, dataProcessor, args, dict_args = get_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_bert.html#BertForMaskedLM\n",
    "\n",
    "https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_bert.html#BertForSequenceClassification    \n",
    "\n",
    "https://huggingface.co/transformers/_modules/transformers/models/roberta/modeling_roberta.html#RobertaForMaskedLM\n",
    "\n",
    "https://huggingface.co/transformers/_modules/transformers/models/distilbert/modeling_distilbert.html#DistilBertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.activations import gelu\n",
    "\n",
    "class BaseModelDistillBert(pl.LightningModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self._frozen = False\n",
    "        \n",
    "        config = AutoConfig.from_pretrained(self.hparams.pretrained,                                            \n",
    "                                            output_attentions=False,\n",
    "                                            output_hidden_states=False)\n",
    "        self.config=config\n",
    "        \n",
    "        A = AutoModel \n",
    "        self.base_model = A.from_pretrained(self.hparams.pretrained, config=config)                \n",
    "                \n",
    "        self.vocab_transform = nn.Linear(config.dim, config.dim)\n",
    "        self.vocab_layer_norm = nn.LayerNorm(config.dim, eps=1e-12)\n",
    "        self.vocab_projector = nn.Linear(config.dim, config.vocab_size)\n",
    "        \n",
    "        self.pre_classifier = nn.Linear(config.dim, config.dim)\n",
    "        self.dropout = nn.Dropout(config.seq_classif_dropout)\n",
    "        \n",
    "        self.CELoss = CrossEntropyLoss()\n",
    "\n",
    "        print('Base: ', type(self.base_model))\n",
    "    \n",
    "\n",
    "    def forward(self, batch):\n",
    "        \n",
    "        outputs = self.base_model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask']\n",
    "        )        \n",
    "        \n",
    "        hidden_states = outputs[0]  # (bs, seq_length, dim)\n",
    "        \n",
    "        masked_lm_loss=None\n",
    "        if 'labels' in batch:        \n",
    "            labels=batch['labels']      \n",
    "            \n",
    "            prediction_logits = self.vocab_transform(hidden_states)  # (bs, seq_length, dim)\n",
    "            prediction_logits = gelu(prediction_logits)  # (bs, seq_length, dim)\n",
    "            prediction_logits = self.vocab_layer_norm(prediction_logits)  # (bs, seq_length, dim)\n",
    "            prediction_logits = self.vocab_projector(prediction_logits)  # (bs, seq_length, vocab_size)\n",
    "\n",
    "            masked_lm_loss = self.CELoss(prediction_logits.view(-1, prediction_logits.size(-1)), labels.view(-1))\n",
    "            \n",
    "            del labels, prediction_logits\n",
    "        \n",
    "        \n",
    "        pooled_output = hidden_states[:, 0]  # (bs, dim)\n",
    "        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
    "        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n",
    "        pooled = self.dropout(pooled_output)  # (bs, dim)\n",
    "\n",
    "        del batch\n",
    "        del outputs\n",
    "        \n",
    "        if masked_lm_loss is not None: \n",
    "            masked_lm_loss = masked_lm_loss.view(1)\n",
    "        \n",
    "        return (masked_lm_loss, pooled)\n",
    "        \n",
    "    \n",
    "# #----------------------------\n",
    "# parser = ArgumentParser()\n",
    "# parser.add_argument('--pretrained', type=str, default=\"distilbert-base-uncased\")\n",
    "# parser.add_argument('--batch_size', type=int, default=32)\n",
    "# parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "# args = parser.parse_args()\n",
    "# dict_args = vars(args)\n",
    "# base_model=BaseModelDistillBert(**dict_args)\n",
    "# print(base_model)\n",
    "# #----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(pl.LightningModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self._frozen = False\n",
    "        \n",
    "        config = AutoConfig.from_pretrained(self.hparams.pretrained,                                            \n",
    "                                            output_attentions=False,\n",
    "                                            output_hidden_states=False)\n",
    "        self.config=config\n",
    "        \n",
    "        A = AutoModel #AutoModelForMaskedLM, AutoModelForSequenceClassification\n",
    "        self.base_model = A.from_pretrained(self.hparams.pretrained, config=config)                \n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "        if self.hparams.pretrained in ['bert-base-uncased','bert-large-uncased']: \n",
    "            self.lm_cls = BertOnlyMLMHead(config)\n",
    "                    \n",
    "        elif self.hparams.pretrained in ['roberta-base','roberta-large']:\n",
    "            self.lm_cls = RobertaLMHead(config)\n",
    "                    \n",
    "        print(\"LM: \",type(self.lm_cls))\n",
    "        \n",
    "        self.CELoss = CrossEntropyLoss()\n",
    "\n",
    "        print('Base: ', type(self.base_model))\n",
    "        \n",
    "        print(\"Freezing Layers:-\")\n",
    "        if self.hparams.freeze=='True':                \n",
    "            if self.hparams.pretrained in ['bert-base-uncased','roberta-base']:\n",
    "                self.freeze('layer.9')\n",
    "            elif self.hparams.pretrained in ['bert-large-uncased', 'roberta-large']:\n",
    "                self.freeze('layer.21')            \n",
    "            else:\n",
    "                print(\"Nothing Frozen\")\n",
    "        elif self.hparams.freeze=='False':\n",
    "            print(\"Nothing Frozen\")\n",
    "        \n",
    "        else:\n",
    "            self.freeze(self.hparams.freeze)\n",
    "                        \n",
    "\n",
    "    def forward(self, batch):\n",
    "        \n",
    "        outputs = self.base_model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask']\n",
    "        )        \n",
    "        \n",
    "        masked_lm_loss=None\n",
    "        if 'labels' in batch:        \n",
    "            labels=batch['labels']      \n",
    "            prediction_scores = self.lm_cls(outputs.last_hidden_state)            \n",
    "            masked_lm_loss = self.CELoss(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "            del labels, prediction_scores\n",
    "        \n",
    "        pooled=outputs.pooler_output        \n",
    "        pooled=self.dropout(pooled)\n",
    "        \n",
    "        del batch\n",
    "        del outputs\n",
    "        \n",
    "        if masked_lm_loss is not None: \n",
    "            masked_lm_loss = masked_lm_loss.view(1)\n",
    "        \n",
    "        return (masked_lm_loss, pooled)\n",
    "    \n",
    "    def freeze(self,layername) -> None:        \n",
    "        for name, param in self.base_model.named_parameters():            \n",
    "#             print(name)            \n",
    "            if layername in name:\n",
    "                print(\"Froze upto: \", name)\n",
    "                break\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        self._frozen = True\n",
    "        \n",
    "    \n",
    "# #----------------------------\n",
    "# parser = ArgumentParser()\n",
    "# parser.add_argument('--pretrained', type=str, default=\"roberta-base\")\n",
    "# parser.add_argument('--batch_size', type=int, default=32)\n",
    "# parser.add_argument('-f') ##dummy for jupyternotebook\n",
    "# args = parser.parse_args()\n",
    "# dict_args = vars(args)\n",
    "# base_model=BaseModel(**dict_args)\n",
    "# print(base_model)\n",
    "# #----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkPredictionModel(pl.LightningModule):\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        #self.lc_1 = nn.Linear(config.hidden_size*2,2)\n",
    "        #self.lc_1 = nn.Linear(config.hidden_size,2)\n",
    "        \n",
    "        self.lc_1 = nn.Linear(config.hidden_size*2, config.hidden_size)\n",
    "        self.lc_2 = nn.Linear(config.hidden_size,2)\n",
    "        \n",
    "        self.tanh=nn.Tanh()\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        self.CELoss = CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, CVE_vectors, CWE_vectors, true_links=None):\n",
    "        logits = self.lc_1(torch.cat((torch.abs(CVE_vectors-CWE_vectors),CVE_vectors*CWE_vectors), 1))\n",
    "        #logits = self.lc_1(CVE_vectors*CWE_vectors)\n",
    "        logits = self.lc_2(self.tanh(logits))\n",
    "        \n",
    "        loss=None\n",
    "        if true_links is not None:\n",
    "            loss=self.CELoss(logits,true_links)     \n",
    "            \n",
    "        if loss is not None: \n",
    "            loss = loss.view(1)\n",
    "            \n",
    "        return (loss, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self,*args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() \n",
    "        \n",
    "        if self.hparams.pretrained == \"distilbert-base-uncased\":\n",
    "            self.base_model=BaseModelDistillBert(*args, **kwargs)\n",
    "        else:        \n",
    "            self.base_model=BaseModel(*args, **kwargs)\n",
    "        self.link_model=LinkPredictionModel(self.base_model.config, *args, **kwargs)\n",
    "        \n",
    "        if NUM_GPUS > 1: \n",
    "            ids=None\n",
    "            if self.hparams.num_gpus!=-1:\n",
    "                ids=list(range(self.hparams.num_gpus))\n",
    "                \n",
    "            self.base_model = nn.DataParallel(self.base_model, device_ids=ids)\n",
    "            self.link_model = nn.DataParallel(self.link_model, device_ids=ids)\n",
    "        \n",
    "\n",
    "    def forward(self, batch, CWE_pooled):                \n",
    "        lm_loss, CVE_pooled=model.base_model(batch)\n",
    "        \n",
    "        CVE_vectors=CVE_pooled[batch['CVE_index']]\n",
    "        CWE_vectors=CWE_pooled[batch['CWE_index']]\n",
    "        true_links=batch['true_labels']\n",
    "    \n",
    "        (loss, logits)=self.link_model(CVE_vectors,CWE_vectors, true_links)        \n",
    "\n",
    "        del CVE_vectors, CWE_vectors, batch\n",
    "        \n",
    "        loss=loss.mean()\n",
    "        \n",
    "        if lm_loss is not None:\n",
    "            loss+= ((self.hparams.lm_lambda)*lm_loss.mean())\n",
    "\n",
    "        return (loss, logits, true_links)\n",
    "\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "        optimizer_grouped_parameters = [{\n",
    "            'params': [\n",
    "                p for n, p in self.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            'weight_decay':\n",
    "            0.01\n",
    "        }, {\n",
    "            'params': [\n",
    "                p for n, p in self.named_parameters()\n",
    "                if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            'weight_decay':\n",
    "            0.0\n",
    "        }]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                          lr=self.hparams.learning_rate,\n",
    "                          eps=1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                          )\n",
    "\n",
    "        \n",
    "        # We also use a scheduler that is supplied by transformers.\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=0, # Default value in run_glue.py\n",
    "            num_training_steps=self.hparams.num_training_steps)\n",
    "\n",
    "        return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printModelParams(model):\n",
    "    print (model)\n",
    "    # Get all of the model's parameters as a list of tuples.\n",
    "    params = list(model.named_parameters())\n",
    "    print('The model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "    print('==== Embedding Layer ====\\n')\n",
    "    for p in params[0:5]:\n",
    "        print(\"{:<55} {:>12}, {}\".format(p[0], str(tuple(p[1].size())),p[1].requires_grad))\n",
    "\n",
    "    print('\\n==== First Transformer ====\\n')\n",
    "    for p in params[5:21]:\n",
    "        print(\"{:<55} {:>12}, {}\".format(p[0], str(tuple(p[1].size())),p[1].requires_grad))\n",
    "\n",
    "    print('\\n==== Output Layer ====\\n')\n",
    "    for p in params[-5:]:\n",
    "        print(\"{:<55} {:>12}, {}\".format(p[0], str(tuple(p[1].size())),p[1].requires_grad))\n",
    "        \n",
    "def print_model_value(model):\n",
    "    params = list(model.named_parameters())\n",
    "    print (params[-1][0],params[-1][1][:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main\n",
    "Two key aspects:\n",
    "\n",
    "- pytorch lightning can add arguments to the parser automatically\n",
    "- you can manually add your own specific arguments.\n",
    "\n",
    "- there is a little more code than seems necessary, because of a particular argument the scheduler\n",
    "  needs. There is currently an open issue on this complication\n",
    "  https://github.com/PyTorchLightning/pytorch-lightning/issues/1038"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "POS/NEG Links:  120\n",
      "BATCH SIZE:  32\n",
      "PRETRAINED:distilbert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/das90/.conda/envs/cent7/2020.11-py38/py38cu11/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:360: UserWarning: Checkpoint directory ./Results/NVD/Model/ exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: <class 'transformers.models.distilbert.tokenization_distilbert_fast.DistilBertTokenizerFast'>\n",
      "Train size: 8\n",
      "Val size: 13\n",
      "Test size: 13\n",
      "Class size: 4\n",
      "USING GPUS: -1\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base:  <class 'transformers.models.distilbert.modeling_distilbert.DistilBertModel'>\n"
     ]
    }
   ],
   "source": [
    "trainer, dataProcessor, args, dict_args = get_configuration()\n",
    "log_results(args.MODEL_DIR_FILE+'_log.txt',dict_args,append=False)\n",
    "\n",
    "model = Model(**dict_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pretrained Model:  ./Results/NVD/Model/CBERT-distilbert-base-uncased.ckpt\n",
      "Model:  <class 'transformers.models.distilbert.modeling_distilbert.DistilBertForMaskedLM'>\n",
      "Matched:  100\n"
     ]
    }
   ],
   "source": [
    "class PretrainedModel(pl.LightningModule):\n",
    "    def __init__(self,*args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        config = AutoConfig.from_pretrained(self.hparams.pretrained,\n",
    "                                            output_attentions=False,\n",
    "                                            output_hidden_states=False)\n",
    "        A = AutoModelForMaskedLM\n",
    "        self.model = A.from_pretrained(self.hparams.pretrained, config=config)\n",
    "        print('Model: ', type(self.model))\n",
    "\n",
    "if args.use_pretrained=='True':\n",
    "    print('Loading Pretrained Model: ',args.PRE_TRAINED_MODEL)    \n",
    "    if os.path.exists(args.PRE_TRAINED_MODEL): \n",
    "            \n",
    "        pretrainedModel=PretrainedModel(**dict_args)\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            if args.pretrained in ['bert-base-uncased','bert-large-uncased']:        \n",
    "\n",
    "                checkpoint = torch.load(args.PRE_TRAINED_MODEL, map_location=lambda storage, loc: storage)\n",
    "\n",
    "                if NUM_GPUS>1: \n",
    "                    model_dict = (model.base_model.module.base_model).state_dict()\n",
    "                    pretrained_dict = {k: v for k, v in (pretrainedModel.model.bert).state_dict().items() if k in model_dict}\n",
    "                    print(\"Matched: \",len(pretrained_dict.keys()))\n",
    "                    model_dict.update(pretrained_dict) \n",
    "                    (model.base_model.module.base_model).load_state_dict(model_dict)\n",
    "                else:\n",
    "                    model_dict = (model.base_model.base_model).state_dict()\n",
    "                    pretrained_dict = {k: v for k, v in (pretrainedModel.model.bert).state_dict().items() if k in model_dict}\n",
    "                    print(\"Matched: \",len(pretrained_dict.keys()))\n",
    "                    model_dict.update(pretrained_dict) \n",
    "                    (model.base_model.base_model).load_state_dict(model_dict)\n",
    "\n",
    "            elif args.pretrained in ['roberta-base','roberta-large']:\n",
    "\n",
    "                checkpoint = torch.load(args.PRE_TRAINED_MODEL, map_location=lambda storage, loc: storage)\n",
    "\n",
    "                if NUM_GPUS>1: \n",
    "                    model_dict = (model.base_model.module.base_model).state_dict()\n",
    "                    pretrained_dict = {k: v for k, v in (pretrainedModel.model.roberta).state_dict().items() if k in model_dict}\n",
    "                    print(\"Matched: \",len(pretrained_dict.keys()))\n",
    "                    model_dict.update(pretrained_dict) \n",
    "                    (model.base_model.module.base_model).load_state_dict(model_dict)\n",
    "                else:\n",
    "                    model_dict = (model.base_model.base_model).state_dict()\n",
    "                    pretrained_dict = {k: v for k, v in (pretrainedModel.model.roberta).state_dict().items() if k in model_dict}\n",
    "                    print(\"Matched: \",len(pretrained_dict.keys()))\n",
    "                    model_dict.update(pretrained_dict) \n",
    "                    (model.base_model.base_model).load_state_dict(model_dict)\n",
    "\n",
    "\n",
    "            elif args.pretrained=='distilbert-base-uncased':\n",
    "\n",
    "                if NUM_GPUS>1: \n",
    "                    model_dict = (model.base_model.module.base_model).state_dict()\n",
    "                    pretrained_dict = {k: v for k, v in (pretrainedModel.model.distilbert).state_dict().items() if k in model_dict}\n",
    "                    print(\"Matched: \",len(pretrained_dict.keys()))\n",
    "                    model_dict.update(pretrained_dict) \n",
    "                    (model.base_model.module.base_model).load_state_dict(model_dict)\n",
    "                else:\n",
    "                    model_dict = (model.base_model.base_model).state_dict()\n",
    "                    pretrained_dict = {k: v for k, v in (pretrainedModel.model.distilbert).state_dict().items() if k in model_dict}\n",
    "                    print(\"Matched: \",len(pretrained_dict.keys()))\n",
    "                    model_dict.update(pretrained_dict) \n",
    "                    (model.base_model.base_model).load_state_dict(model_dict)\n",
    "        except:\n",
    "            print(\"Pytorch version missmatch between saved and new model\")\n",
    "\n",
    "    else:\n",
    "        print(\"File not found will continue with original model\")\n",
    "else:\n",
    "    print(\"Use default model as base...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depolying model to  cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"Depolying model to \",device)\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader=dataProcessor.train_dataloader()    \n",
    "val_dataloader=dataProcessor.val_dataloader()\n",
    "test_dataloader=dataProcessor.test_dataloader()\n",
    "class_dataloader=dataProcessor.class_dataloader()\n",
    "\n",
    "train_dataloaderNC=dataProcessor.train_dataloader(use_collator=False)    \n",
    "val_dataloaderNC=dataProcessor.val_dataloader(use_collator=False)\n",
    "test_dataloaderNC=dataProcessor.test_dataloader(use_collator=False)\n",
    "class_dataloaderNC=dataProcessor.class_dataloader(use_collator=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter_dataloader=iter(class_dataloaderNC)\n",
    "# next(iter_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def th_link_acc(preds,labels,th=0.50):\n",
    "    pred=(preds >= th).astype(int)    \n",
    "    return np.sum(pred == labels) / len(labels)\n",
    "\n",
    "def th_link_f1_score(logits,y_true,th=0.50):\n",
    "    logits=(torch.nn.functional.softmax(logits,dim=1))[:1]\n",
    "    y_pred=(logits >= th).astype(int)    \n",
    "    \n",
    "    return f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "def link_f1_score(logits,y_true):\n",
    "#     print(logits)\n",
    "#     logits=(torch.nn.functional.softmax(logits,dim=1))\n",
    "    y_pred=np.argmax(logits,axis=1)\n",
    "#     print(y_pred)\n",
    "#     print(y_true)\n",
    "    \n",
    "    return f1_score(y_true, y_pred, average='macro')\n",
    "#     f1_score(y_true, y_pred, average='micro')\n",
    "#     f1_score(y_true, y_pred, average='weighted')    \n",
    "#     f1_score(y_true, y_pred, zero_division=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,j in dataProcessor.levels.items():\n",
    "#     print(i,\"->\",len(j),':',j)\n",
    "levels=dataProcessor.levels\n",
    "data=dataProcessor.data\n",
    "\n",
    "def getHierIndexs(child, preds,k2, indexs, rec=-1):    \n",
    "    if child in data.parent_child:\n",
    "        children=data.parent_child[child]           \n",
    "        child_preds=preds[children]\n",
    "\n",
    "        k_child=min(len(children),k2)\n",
    "        child_indexs=np.argpartition(child_preds, -k_child)[-k_child:]                \n",
    "        child_index_map=dict(zip(range(len(children)),children))\n",
    "        \n",
    "        grandchildren=[child_index_map[ix] for ix in child_indexs]\n",
    "        \n",
    "        indexs.extend(grandchildren)\n",
    "    \n",
    "        if(rec==-1):\n",
    "            for grandchild in grandchildren:\n",
    "                getHierIndexs(grandchild,preds,k2, indexs)\n",
    "    \n",
    "\n",
    "def getPrediction(firstpredictions,preds,k1,k2):\n",
    "    \n",
    "    results=[]\n",
    "    \n",
    "    for child in firstpredictions:\n",
    "\n",
    "        childrens=[]\n",
    "        getHierIndexs(child, preds, k1, childrens, rec=1)\n",
    "        \n",
    "        results.append(child)\n",
    "        results.extend(childrens)\n",
    "\n",
    "        for grandchild in childrens:\n",
    "            grandchildren=[]\n",
    "            getHierIndexs(grandchild,preds,k2,grandchildren)\n",
    "            results.extend(grandchildren)\n",
    "    \n",
    "    return results\n",
    "    \n",
    "\n",
    "def top_k_accuracy(preds, true_labels, k0=1,k1=1,k2=1):\n",
    "    n0_level=levels[0]\n",
    "    n0_level_preds=preds[:,n0_level]\n",
    "    index_map=dict(zip(range(len(n0_level)),n0_level))\n",
    "    \n",
    "    k0=min(len(n0_level),k0)\n",
    "    top_k0_indexs= np.argpartition(n0_level_preds, -k0)[:,-k0:]\n",
    "    org_k0_indexs= [[index_map[j] for j in i] for i in top_k0_indexs]\n",
    "    \n",
    "    #preds_k=Parallel(n_jobs=num_processors)(delayed(getPrediction)(org_k0_indexs[i],preds[i,:],k1,k2) for i in range(len(org_k0_indexs)))    \n",
    "    preds_k=[getPrediction(org_k0_indexs[i],preds[i,:],k1,k2) for i in range(len(org_k0_indexs))]    \n",
    "    clusters=[np.where(t_label == 1)[0] for t_label in true_labels]\n",
    "    \n",
    "    corrects=[bool(set(clusters[i]) & set(preds_k[i])) for i in range(len(preds_k))]\n",
    "    \n",
    "    return sum(corrects)/len(corrects)\n",
    "\n",
    "# preds=np.array([\n",
    "#     [0.1,0.7,0.4,0.6],\n",
    "#     [0,1,0,0],\n",
    "#     [0.3,0.1,0,0.4]])\n",
    "# labels=np.array([\n",
    "#     [1,0,0,0],\n",
    "#     [0,1,0,0],\n",
    "#     [0,0,0,1]])\n",
    "\n",
    "# print(top_k_accuracy(preds,labels,k0=1,k1=1,k2=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_links(batch):    \n",
    "    POS=batch['pos']\n",
    "    NEG=batch['neg']\n",
    "    POS_label=batch['pos_label']\n",
    "    NEG_label=batch['neg_label']\n",
    "    CWE_index=torch.cat((POS.view(-1),NEG.view(-1)))    \n",
    "    CVE_index=torch.cat((torch.tensor(np.repeat(range(POS.shape[0]),K_NEG_LINKS),dtype=torch.long),\n",
    "               torch.tensor(np.repeat(range(NEG.shape[0]),K_NEG_LINKS),dtype=torch.long)))\n",
    "    \n",
    "    true_links=torch.cat((POS_label.view(-1),NEG_label.view(-1)))\n",
    "    \n",
    "    return CVE_index, CWE_index, true_links\n",
    "\n",
    "def prepare_all_links(nCVEs=2,nCWEs=3):    \n",
    "    CVE_index=torch.tensor(np.repeat(range(nCVEs),nCWEs),dtype=torch.long)\n",
    "    CWE_index=torch.tensor(np.tile(range(nCWEs), nCVEs),dtype=torch.long)    \n",
    "    return CVE_index, CWE_index\n",
    "\n",
    "# prepare_all_links()\n",
    "# iter_train_dataloader=iter(dataProcessor.train_dataloader())\n",
    "# batch=next(iter_train_dataloader)\n",
    "# print(batch)\n",
    "# prepare_links(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute accuracy hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def LINK_evaluate_model(dataloader, c_dataloader):\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    eval_accuracies=np.zeros(len(TOP_K0))\n",
    "    \n",
    "    num_batches=len(dataloader)\n",
    "    total_loss = 0\n",
    "    loss_value = -1\n",
    "    nb_steps=0\n",
    "    nb_links=0\n",
    "    step_time=0\n",
    "    total_step_time=0\n",
    "    current_time=0\n",
    "    epoch_start=time.time()\n",
    "    total_acc=0\n",
    "    step_acc=0\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        class_batch=next(iter(c_dataloader))\n",
    "        nCWEs=class_batch['o_labels'].shape[0]\n",
    "        \n",
    "        for key,value in class_batch.items(): \n",
    "            class_batch[key]=value.to(device)\n",
    "        \n",
    "        (_, CWE_pooled)=model.base_model(class_batch) #0-classlmloss, 1-classpooled \n",
    "        \n",
    "        for step, batch in enumerate(dataloader):\n",
    "            step_start=time.time()\n",
    "            if step % args.refresh_rate == 0:            \n",
    "                print(\n",
    "                    'Batch {:}/{:} - {:0.4f} s/it, {:0.4f} s - Elapsed: {:0.4f} s'.format(\n",
    "                    step,\n",
    "                    num_batches,\n",
    "                    step_time,\n",
    "                    total_step_time,\n",
    "                    time.time()-epoch_start)\n",
    "                )\n",
    "            \n",
    "            nCVEs=batch['o_labels'].shape[0]            \n",
    "            batch['nCVEs'], batch['nCWEs']=prepare_all_links(nCVEs,nCWEs)            \n",
    "            for key,value in batch.items(): batch[key]=value.to(device)\n",
    "            \n",
    "            (_, CVE_pooled) = model.base_model(batch)\n",
    "            \n",
    "            CVE_vectors=CVE_pooled[batch['nCVEs']]\n",
    "            CWE_vectors=CWE_pooled[batch['nCWEs']]\n",
    "            \n",
    "            (_, logits)=model.link_model(CVE_vectors,CWE_vectors)\n",
    "            \n",
    "            logits=(torch.nn.functional.softmax(logits,dim=1))[:,1]\n",
    "            logits=logits.view(nCVEs,-1)\n",
    "            loggits=logits.detach().cpu().numpy()\n",
    "            true_labels=batch['o_labels'].cpu().numpy()\n",
    "            \n",
    "            step_time=time.time()-step_start\n",
    "            total_step_time+=step_time\n",
    "            \n",
    "            for k_i in range(len(TOP_K0)):\n",
    "                tmp_eval=top_k_accuracy(loggits, true_labels, TOP_K0[k_i], TOP_K1[k_i], TOP_K2[k_i])*len(logits)\n",
    "                eval_accuracies[k_i]+=tmp_eval\n",
    "                \n",
    "            nb_eval_examples+=len(logits)            \n",
    "            \n",
    "    eval_accuracies=eval_accuracies/nb_eval_examples\n",
    "        \n",
    "    print(\"-\"*25)\n",
    "    for k_i in range(len(TOP_K0)):\n",
    "        print(\" Top {0},{1},{2}... Accuracy: {3:.4f}\".format(TOP_K0[k_i],TOP_K1[k_i],TOP_K2[k_i], eval_accuracies[k_i]))\n",
    "    print(\"-\"*25)\n",
    "    \n",
    "    return eval_accuracies\n",
    "\n",
    "#LINK_evaluate_model(val_dataloaderNC, class_dataloaderNC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate_links(dataloader, c_dataloader):\n",
    "    num_batches=len(dataloader)\n",
    "    \n",
    "    total_loss = 0\n",
    "    loss_value = -1\n",
    "    nb_steps=0\n",
    "    nb_links=0\n",
    "    step_time=0\n",
    "    total_step_time=0\n",
    "    current_time=0\n",
    "    epoch_start=time.time()\n",
    "    total_acc=0\n",
    "    step_acc=0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        class_batch=next(iter(c_dataloader))\n",
    "        for key,value in class_batch.items(): \n",
    "            class_batch[key]=value.to(device)\n",
    "        \n",
    "        class_outputs=model.base_model(class_batch) #0-classlmloss, 1-classpooled\n",
    "        class_lm_loss=class_outputs[0]\n",
    "        \n",
    "        if class_lm_loss is not None:\n",
    "            total_loss+=(args.lm_lambda)*(class_lm_loss.mean()).item()\n",
    "\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            step_start=time.time()\n",
    "            if step % args.refresh_rate == 0:            \n",
    "                print(\n",
    "                    'Batch {:}/{:} - {:0.4f} s/it, {:0.4f} s - Elapsed: {:0.4f} s, loss_step {:0.4f}, loss_epoch {:0.4f} - eval_f1_step {:0.4f}, eval_f1_epoch {:0.4f}'.format(\n",
    "                    step,\n",
    "                    num_batches,\n",
    "                    step_time,\n",
    "                    total_step_time,\n",
    "                    time.time()-epoch_start, \n",
    "                    loss_value,\n",
    "                    total_loss/max(nb_steps,1),\n",
    "                    step_acc,\n",
    "                    total_acc/max(nb_links,1)))\n",
    "\n",
    "            batch['CVE_index'], batch['CWE_index'], batch['true_labels']=prepare_links(batch)       \n",
    "            for key,value in batch.items(): batch[key]=value.to(device)\n",
    "            \n",
    "            outputs = model(batch, class_outputs[1]) #0-loss, 1-logits, 2-true-links            \n",
    "            \n",
    "            loss = outputs[0].mean()\n",
    "\n",
    "            step_time=time.time()-step_start\n",
    "            total_step_time+=step_time\n",
    "\n",
    "            loss_value = loss.item()\n",
    "            total_loss+=loss_value\n",
    "            logits=(torch.nn.functional.softmax(outputs[1].detach(),dim=1))\n",
    "            logits= logits.cpu().numpy()\n",
    "            true_links=outputs[2].detach().cpu().numpy()\n",
    "\n",
    "            nb_steps+=1\n",
    "            nb_links+=len(true_links)\n",
    "\n",
    "            step_acc=link_f1_score(logits,true_links)\n",
    "            total_acc+=step_acc*len(true_links)\n",
    "    \n",
    "    eval_loss=total_loss/nb_steps\n",
    "    eval_accuracy=total_acc/nb_links\n",
    "    \n",
    "    return eval_loss, eval_accuracy\n",
    "\n",
    "#Evaluate_links(val_dataloaderNC, class_dataloaderNC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0044 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6942\n",
      "Train F1-Score: 0.4286\n",
      "Train time: 3.0160 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0512 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.7500\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0174 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6893\n",
      " Eval F1-Score: 0.3333\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0174 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.6923\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Saving model....acc: 0.6923076923076923\n",
      "Epoch 1/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0032 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6978\n",
      "Train F1-Score: 0.4946\n",
      "Train time: 0.2059 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0157 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.7500\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0168 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6856\n",
      " Eval F1-Score: 0.4847\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0166 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8462\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Saving model....acc: 0.8461538461538461\n",
      "Epoch 2/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0032 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6902\n",
      "Train F1-Score: 0.5728\n",
      "Train time: 0.1976 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.1234 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.8750\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0170 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6818\n",
      " Eval F1-Score: 0.6515\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0166 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 0.9231\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Saving model....acc: 0.9230769230769231\n",
      "Epoch 3/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0034 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6858\n",
      "Train F1-Score: 0.5555\n",
      "Train time: 0.1974 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0160 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0171 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6779\n",
      " Eval F1-Score: 0.8373\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0173 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Saving model....acc: 1.0\n",
      "Epoch 4/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0034 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6865\n",
      "Train F1-Score: 0.4984\n",
      "Train time: 0.1973 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0167 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0170 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6738\n",
      " Eval F1-Score: 0.8526\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0170 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 5/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0026 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6815\n",
      "Train F1-Score: 0.4250\n",
      "Train time: 0.1958 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0162 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0169 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6695\n",
      " Eval F1-Score: 0.8459\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0171 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 6/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0028 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6799\n",
      "Train F1-Score: 0.5474\n",
      "Train time: 0.1966 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0185 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0183 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6653\n",
      " Eval F1-Score: 0.8649\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0178 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 7/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0029 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6790\n",
      "Train F1-Score: 0.4589\n",
      "Train time: 0.1953 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0160 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0169 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6610\n",
      " Eval F1-Score: 0.8776\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0172 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 8/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0027 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6737\n",
      "Train F1-Score: 0.6178\n",
      "Train time: 0.1953 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0163 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0173 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6566\n",
      " Eval F1-Score: 0.8903\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0167 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 9/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0028 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6651\n",
      "Train F1-Score: 0.6537\n",
      "Train time: 0.1960 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0162 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0169 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6521\n",
      " Eval F1-Score: 0.8903\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0164 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 10/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0032 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6661\n",
      "Train F1-Score: 0.6537\n",
      "Train time: 0.1954 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0164 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0171 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6475\n",
      " Eval F1-Score: 0.8903\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0165 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 11/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0028 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6655\n",
      "Train F1-Score: 0.4589\n",
      "Train time: 0.1965 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0163 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0170 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6429\n",
      " Eval F1-Score: 0.8776\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0168 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 12/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0026 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6579\n",
      "Train F1-Score: 0.7312\n",
      "Train time: 0.1958 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0160 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0164 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6384\n",
      " Eval F1-Score: 0.8776\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0170 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 13/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0029 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6587\n",
      "Train F1-Score: 0.7506\n",
      "Train time: 0.2018 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0184 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0162 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6339\n",
      " Eval F1-Score: 0.8776\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0168 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 14/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0027 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6625\n",
      "Train F1-Score: 0.4589\n",
      "Train time: 0.1958 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0158 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0169 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6299\n",
      " Eval F1-Score: 0.8776\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0167 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 15/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0027 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6441\n",
      "Train F1-Score: 0.8322\n",
      "Train time: 0.2056 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0161 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0167 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6260\n",
      " Eval F1-Score: 0.8776\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0168 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 16/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0028 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6386\n",
      "Train F1-Score: 0.9054\n",
      "Train time: 0.1952 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0160 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0169 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6221\n",
      " Eval F1-Score: 0.8776\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0164 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 17/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0026 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6348\n",
      "Train F1-Score: 0.8958\n",
      "Train time: 0.1957 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0161 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0174 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6185\n",
      " Eval F1-Score: 0.8776\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0179 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 18/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0026 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6343\n",
      "Train F1-Score: 0.8196\n",
      "Train time: 0.1951 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0160 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0170 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6153\n",
      " Eval F1-Score: 0.8776\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0164 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 19/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0028 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6362\n",
      "Train F1-Score: 0.7857\n",
      "Train time: 0.1960 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0179 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0177 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6123\n",
      " Eval F1-Score: 0.8776\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0165 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 20/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0031 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6291\n",
      "Train F1-Score: 0.8526\n",
      "Train time: 0.1958 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0158 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0172 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6098\n",
      " Eval F1-Score: 0.8776\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0161 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 21/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0026 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6308\n",
      "Train F1-Score: 0.6946\n",
      "Train time: 0.1964 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0167 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0166 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6077\n",
      " Eval F1-Score: 0.8776\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0169 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 22/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0028 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6271\n",
      "Train F1-Score: 0.7703\n",
      "Train time: 0.1957 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0157 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0170 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6061\n",
      " Eval F1-Score: 0.8378\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0167 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 23/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0028 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6241\n",
      "Train F1-Score: 0.7659\n",
      "Train time: 0.1955 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0168 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0182 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6050\n",
      " Eval F1-Score: 0.8776\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0167 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Epoch 24/25 Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0029 s, loss_step -1.0000, loss_epoch 0.0000 - train_f1_step 0.0000, train_f1_epoch 0.0000\n",
      "Train loss: 0.6234\n",
      "Train F1-Score: 0.9165\n",
      "Train time: 0.1960 sec\n",
      "Evaluate train model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0158 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Validation.....\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0173 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average evaluation loss: 0.6045\n",
      " Eval F1-Score: 0.8776\n",
      "Evaluate validation model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0174 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Link Prediction Training complete!\n",
      "Saving Last model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0224 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      " Average test loss: 0.6045\n",
      "Evaluate test model\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0197 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train_model(dataloader, c_dataloader, v_dataloader=None, t_dataloader=None):    \n",
    "    optimizer, scheduler = model.configure_optimizers()\n",
    "    num_batches=len(dataloader)\n",
    "    \n",
    "    train_losses=[]\n",
    "    train_accs=[]\n",
    "    val_losses=[]\n",
    "    val_accs=[]\n",
    "    train_class_acc=[]\n",
    "    val_class_acc=[]\n",
    "    best_val=-1\n",
    "    \n",
    "    total_train_time=0\n",
    "    \n",
    "    for epoch_i in range(args.epochs):\n",
    "        \n",
    "        total_loss = 0\n",
    "        loss_value = -1\n",
    "        nb_steps=0\n",
    "        nb_links=0\n",
    "        step_time=0\n",
    "        total_step_time=0\n",
    "        current_time=0\n",
    "        epoch_start=time.time()\n",
    "        total_acc=0\n",
    "        step_acc=0\n",
    "        \n",
    "        model.train()\n",
    "        class_batch=next(iter(c_dataloader))\n",
    "        for key,value in class_batch.items(): class_batch[key]=value.to(device)\n",
    "            \n",
    "        for step, batch in enumerate(dataloader):\n",
    "            step_start=time.time()\n",
    "            if step % args.refresh_rate == 0:            \n",
    "                print(\n",
    "                    'Epoch {:}/{:} Batch {:}/{:} - {:0.4f} s/it, {:0.4f} s - Elapsed: {:0.4f} s, loss_step {:0.4f}, loss_epoch {:0.4f} - train_f1_step {:0.4f}, train_f1_epoch {:0.4f}'.format(\n",
    "                    epoch_i,\n",
    "                    args.epochs, \n",
    "                    step,\n",
    "                    num_batches,\n",
    "                    step_time,\n",
    "                    total_step_time,\n",
    "                    time.time()-epoch_start, \n",
    "                    loss_value,\n",
    "                    total_loss/max(nb_steps,1),\n",
    "                    step_acc,\n",
    "                    total_acc/max(nb_links,1)))\n",
    "            \n",
    "            batch['CVE_index'], batch['CWE_index'], batch['true_labels']=prepare_links(batch)       \n",
    "            for key,value in batch.items(): batch[key]=value.to(device)\n",
    "                            \n",
    "            model.zero_grad()\n",
    "            class_outputs=model.base_model(class_batch) #0-classlmloss, 1-classpooled\n",
    "            class_lm_loss=class_outputs[0]  \n",
    "            \n",
    "            outputs = model(batch, class_outputs[1]) #0-loss, 1-logits, 2-true-links        \n",
    "            \n",
    "            loss = outputs[0].mean()            \n",
    "            if class_lm_loss is not None:\n",
    "                loss+= (args.lm_lambda)*class_lm_loss.mean()\n",
    "            \n",
    "            loss.backward() \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) \n",
    "            \n",
    "            optimizer.step() \n",
    "            scheduler.step()\n",
    "            \n",
    "            step_time=time.time()-step_start\n",
    "            total_step_time+=step_time\n",
    "            \n",
    "            #stats\n",
    "            loss_value = loss.item()\n",
    "            total_loss+=loss_value\n",
    "            logits=(torch.nn.functional.softmax(outputs[1].detach(),dim=1))\n",
    "            logits= logits.cpu().numpy()\n",
    "            true_links=outputs[2].detach().cpu().numpy()\n",
    "            \n",
    "            nb_steps+=1\n",
    "            nb_links+=len(true_links)\n",
    "            \n",
    "            step_acc=link_f1_score(logits,true_links)\n",
    "            total_acc+=step_acc*len(true_links)\n",
    "            \n",
    "        total_train_time+=total_step_time\n",
    "\n",
    "        avg_train_loss=total_loss/nb_steps\n",
    "        avg_train_acc=total_acc/nb_links\n",
    "        \n",
    "        print(\"Train loss: {0:.4f}\".format(avg_train_loss))\n",
    "        print(\"Train F1-Score: {0:.4f}\".format(avg_train_acc))\n",
    "        print(\"Train time: {0:.4f} sec\".format(total_step_time))\n",
    "        \n",
    "        if args.performance_mode=='True':return\n",
    "    \n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accs.append(avg_train_acc)\n",
    "        \n",
    "        print(\"Evaluate train model\")\n",
    "        train_cwe_acc=LINK_evaluate_model(dataloader,c_dataloader)\n",
    "        train_class_acc.append(train_cwe_acc[0])\n",
    "        #print(train_cwe_acc)\n",
    "        \n",
    "        if v_dataloader is not None:\n",
    "            print(\"Validation.....\")        \n",
    "            eval_loss, eval_accuracy=Evaluate_links(v_dataloader, c_dataloader)      \n",
    "        \n",
    "            print(\" Average evaluation loss: {0:.4f}\".format(eval_loss))\n",
    "            print(\" Eval F1-Score: {0:.4f}\".format(eval_accuracy))\n",
    "        \n",
    "            val_accs.append(eval_accuracy)\n",
    "            val_losses.append(eval_loss)\n",
    "         \n",
    "            print(\"Evaluate validation model\")\n",
    "            val_cwe_acc=LINK_evaluate_model(v_dataloader,c_dataloader)\n",
    "            val_class_acc.append(val_cwe_acc[0])\n",
    "            \n",
    "            if args.checkpointing=='True':\n",
    "                if(best_val is None or val_cwe_acc[0]>best_val):\n",
    "                    best_val=val_cwe_acc[0]\n",
    "                    print(\"Saving model....acc:\",best_val)\n",
    "                    torch.save(model.state_dict(), args.MODEL_DIR_FILE+'_BEST')\n",
    "\n",
    "    print(\"Link Prediction Training complete!\")\n",
    "    \n",
    "    if args.checkpointing=='True':\n",
    "        print(\"Saving Last model\")\n",
    "        torch.save(model.state_dict(), args.MODEL_DIR_FILE+'_LAST')\n",
    "    \n",
    "    if t_dataloader is not None:    \n",
    "        eval_loss, eval_accuracy=Evaluate_links(t_dataloader,c_dataloader)              \n",
    "        print(\" Average test loss: {0:.4f}\".format(eval_loss))\n",
    "\n",
    "        print(\"Evaluate test model\")\n",
    "        test_cwe_acc=LINK_evaluate_model(t_dataloader,c_dataloader)        \n",
    "        log_results(args.MODEL_DIR_FILE+'_log.txt', {'test_accuracies':test_cwe_acc})\n",
    "    \n",
    "    from ipynb.fs.full.lib.Utils import save_plot\n",
    "    save_plot(train_accs, val_accs, name=args.MODEL_DIR_FILE+'_link_acc',yname='Accuracy')\n",
    "    save_plot(train_losses, val_losses, name=args.MODEL_DIR_FILE+'_link_loss',yname='Loss')\n",
    "    save_plot(train_class_acc, val_class_acc, name=args.MODEL_DIR_FILE+'_class_acc',yname='Accuracy')\n",
    "    \n",
    "    log_results(args.MODEL_DIR_FILE+'_log.txt',\n",
    "               {'train_accs':train_accs, 'val_accs':val_accs,\n",
    "                'train_losses':train_losses, 'val_losses':val_losses,\n",
    "                'train_class_acc':train_class_acc, 'val_class_acc':val_class_acc})\n",
    "    \n",
    "    log_results(args.MODEL_DIR_FILE+'_log.txt', {\n",
    "        'total_step_time':[total_step_time],\n",
    "        'num_batches':[num_batches],\n",
    "        'epochs':[args.epochs],\n",
    "        'batch_size':[args.batch_size]})\n",
    "    \n",
    "    return\n",
    "\n",
    "if args.use_rd == 'True':\n",
    "    train_model(train_dataloader, class_dataloader, val_dataloaderNC, test_dataloaderNC)\n",
    "else:\n",
    "    train_model(train_dataloaderNC, class_dataloaderNC, val_dataloaderNC, test_dataloaderNC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Saved Model:  ./Results/NVD/Model/CBERT-LINK-distilbert-base-uncased-dp_LAST\n",
      "Original weights: \n",
      "link_model.module.lc_2.bias tensor([-0.0193,  0.0207], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Loaded weights: \n",
      "link_model.module.lc_2.bias tensor([-0.0193,  0.0207], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0214 s\n",
      "-------------------------\n",
      " Top 1,1,1... Accuracy: 1.0000\n",
      " Top 3,2,1... Accuracy: 1.0000\n",
      " Top 5,2,2... Accuracy: 1.0000\n",
      "-------------------------\n",
      "Batch 0/1 - 0.0000 s/it, 0.0000 s - Elapsed: 0.0201 s, loss_step -1.0000, loss_epoch 0.0000 - eval_f1_step 0.0000, eval_f1_epoch 0.0000\n",
      "Test F-1 Score:  0.8775965310757795\n",
      "Test Complete......\n"
     ]
    }
   ],
   "source": [
    "def test_model(t_dataloader, c_dataloader):\n",
    "    MODEL_NAME=args.MODEL_DIR_FILE+'_LAST'    \n",
    "\n",
    "    if os.path.exists(MODEL_NAME): \n",
    "        print('Loading Saved Model: ',MODEL_NAME)        \n",
    "    else: \n",
    "        print(\"File not found: \",MODEL_NAME)\n",
    "        return\n",
    "    \n",
    "    print(\"Original weights: \");print_model_value(model)\n",
    "    checkpoint = torch.load(MODEL_NAME, map_location=lambda storage, loc: storage)\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "    print(\"Loaded weights: \");print_model_value(model)    \n",
    "    LINK_evaluate_model(test_dataloaderNC, class_dataloaderNC)    \n",
    "    \n",
    "    eval_loss, eval_accuracy = Evaluate_links(test_dataloaderNC, class_dataloaderNC)\n",
    "    print(\"Test F-1 Score: \", eval_accuracy)\n",
    "    \n",
    "    print(\"Test Complete......\")\n",
    "    \n",
    "    return\n",
    "\n",
    "if args.checkpointing=='True':\n",
    "    test_model(test_dataloaderNC, class_dataloaderNC)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
